# Core API keys
APP_ENV=development
LLM_PROVIDER=openai
LLM_MODEL_ID=gpt-4o-mini
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL_ID=gpt-4o-mini
OPENAI_BASE_URL=
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OLLAMA_BASE_URL=http://localhost:11434/v1
ANTHROPIC_API_KEY=
GEMINI_API_KEY=

# Embedding configuration (defaults to LLM_PROVIDER if that provider supports embeddings)
EMBEDDING_PROVIDER=  # openai | openrouter | ollama | gemini | voyage
VOYAGE_API_KEY=  # Required if EMBEDDING_PROVIDER=voyage (useful for Anthropic users)

# Backend database connections
DATABASE_URL=postgresql+psycopg://agentic_rag:agentic_rag@localhost:5432/agentic_rag
DB_POOL_MIN=1
DB_POOL_MAX=50
REQUEST_MAX_BYTES=1048576
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_BACKEND=memory
RATE_LIMIT_REDIS_PREFIX=rate-limit

# Development-only credentials (override in production)
POSTGRES_DB=agentic_rag
POSTGRES_USER=agentic_rag
POSTGRES_PASSWORD=agentic_rag
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j_password
NEO4J_POOL_MIN=1
NEO4J_POOL_MAX=50
NEO4J_POOL_ACQUIRE_TIMEOUT_SECONDS=30
NEO4J_CONNECTION_TIMEOUT_SECONDS=30
NEO4J_MAX_CONNECTION_LIFETIME_SECONDS=3600
REDIS_URL=redis://localhost:6379/0

# Optional app configuration
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
FRONTEND_URL=http://localhost:3000
RATE_LIMIT_RETRY_AFTER_SECONDS=60
# Frontend tenant scope (used by ops/knowledge dashboards)
NEXT_PUBLIC_TENANT_ID=

# Epic 7 - A2A settings (in-memory session limits)
A2A_SESSION_TTL_SECONDS=21600
A2A_CLEANUP_INTERVAL_SECONDS=3600
A2A_MAX_SESSIONS_PER_TENANT=100
A2A_MAX_SESSIONS_TOTAL=1000
A2A_MAX_MESSAGES_PER_SESSION=1000

# Epic 14 - Enhanced A2A Protocol (agent-to-agent communication)
A2A_ENABLED=true
A2A_AGENT_ID=agentic-rag-001
# A2A_ENDPOINT_URL: The base URL where this agent can be reached by other agents.
# Do NOT include /api/v1 suffix - the A2A protocol adds this automatically.
# Example: http://localhost:8000 (correct), NOT http://localhost:8000/api/v1 (incorrect)
A2A_ENDPOINT_URL=http://localhost:8000
A2A_HEARTBEAT_INTERVAL_SECONDS=30
A2A_HEARTBEAT_TIMEOUT_SECONDS=60
A2A_TASK_DEFAULT_TIMEOUT_SECONDS=300
A2A_TASK_MAX_RETRIES=3

# Epic 7 - MCP settings
MCP_TOOL_TIMEOUT_SECONDS=30
MCP_TOOL_TIMEOUT_OVERRIDES={"knowledge.query":30,"knowledge.graph_stats":10}
MCP_TOOL_MAX_TIMEOUT_SECONDS=300

# Epic 8 - Ops settings
# Pricing JSON format: {"gpt-4o-mini":{"input_per_1k":0.00015,"output_per_1k":0.00060}}
MODEL_PRICING_JSON=
ROUTING_SIMPLE_MODEL=gpt-4o-mini
ROUTING_MEDIUM_MODEL=gpt-4o
ROUTING_COMPLEX_MODEL=gpt-4o
ROUTING_BASELINE_MODEL=gpt-4o
ROUTING_SIMPLE_MAX_SCORE=2
ROUTING_COMPLEX_MIN_SCORE=5
TRACE_ENCRYPTION_KEY= # 64 hex chars (32 bytes). Example: python -c "import secrets; print(secrets.token_hex(32))"

# Observability
METRICS_TENANT_LABEL_MODE=global  # full | hash | global
METRICS_TENANT_LABEL_BUCKETS=100  # Used when mode=hash

# Epic 12 - Advanced Retrieval (all features are opt-in)
# Cross-encoder reranking
RERANKER_ENABLED=false
RERANKER_PROVIDER=flashrank  # cohere | flashrank
RERANKER_TOP_K=10
RERANKER_MODEL=  # Optional override (uses provider defaults)
COHERE_API_KEY=  # Required only if RERANKER_PROVIDER=cohere

# Contextual retrieval (chunk enrichment)
CONTEXTUAL_RETRIEVAL_ENABLED=false
CONTEXTUAL_MODEL=claude-3-haiku-20240307  # Cost-effective model recommended
CONTEXTUAL_PROMPT_CACHING=true  # Reduces cost by ~90% (Anthropic only)

# Epic 20 - Small-to-Big retrieval (hierarchical chunking)
HIERARCHICAL_CHUNKS_ENABLED=false
HIERARCHICAL_CHUNK_LEVELS=256,512,1024,2048
HIERARCHICAL_OVERLAP_RATIO=0.1
HIERARCHICAL_EMBEDDING_LEVEL=0  # Level used for embeddings (0 = smallest)
SMALL_TO_BIG_RETURN_LEVEL=2  # Level returned for context (2 = 1024 tokens)

# Epic 20 - Graph-based rerankers
GRAPH_RERANKER_ENABLED=false
GRAPH_RERANKER_TYPE=hybrid  # episode | distance | hybrid
GRAPH_RERANKER_EPISODE_WEIGHT=0.3
GRAPH_RERANKER_DISTANCE_WEIGHT=0.3
GRAPH_RERANKER_ORIGINAL_WEIGHT=0.4
GRAPH_RERANKER_EPISODE_WINDOW_DAYS=30
GRAPH_RERANKER_MAX_DISTANCE=3

# Epic 20 - Dual-level retrieval
DUAL_LEVEL_RETRIEVAL_ENABLED=false
DUAL_LEVEL_LOW_WEIGHT=0.6
DUAL_LEVEL_HIGH_WEIGHT=0.4
DUAL_LEVEL_LOW_LIMIT=10
DUAL_LEVEL_HIGH_LIMIT=5
DUAL_LEVEL_SYNTHESIS_MODEL=gpt-4o-mini
DUAL_LEVEL_SYNTHESIS_TEMPERATURE=0.3

# CRAG grader (relevance scoring with fallback)
GRADER_ENABLED=false
GRADER_THRESHOLD=0.5  # Score below triggers fallback (0.0-1.0)
GRADER_FALLBACK_ENABLED=true
GRADER_FALLBACK_STRATEGY=web_search  # web_search | expanded_query | alternate_index
TAVILY_API_KEY=  # Required for web_search fallback

# Epic 13 - Enterprise Ingestion
# Crawl4AI configuration
CRAWL4AI_PROFILE=fast  # fast | thorough | stealth
CRAWLER_STRICT_VALIDATION=  # Default: true in prod, false in dev
CRAWL4AI_RATE_LIMIT=1.0  # Requests per second
CRAWL4AI_HEADLESS=true
CRAWL4AI_MAX_CONCURRENT=10
CRAWL4AI_CACHE_ENABLED=true
CRAWL4AI_JS_WAIT_SECONDS=2.0
CRAWL4AI_PAGE_TIMEOUT_MS=60000
CRAWL4AI_PROXY_URL=  # Optional proxy for all requests
CRAWL4AI_STEALTH_PROXY=  # Proxy specifically for stealth mode
CRAWL_PROFILE_CONFIG_PATH=config/crawl-profiles.yaml  # Optional domain mapping override
CRAWLER_USER_AGENT_STRATEGY=rotate  # rotate | random | static
CRAWLER_USER_AGENT_LIST_PATH=config/user-agents.txt  # Optional custom list
CRAWLER_USER_AGENT_USE_FAKE=false  # Use fake-useragent if installed (random strategy)
CRAWLER_BLOOM_FILTER_THRESHOLD=10000  # Use bloom filter for large crawls
CRAWLER_BLOOM_FILTER_ERROR_RATE=0.001  # False positive rate for bloom filter

# Fallback providers for anti-bot protected sites
CRAWL_FALLBACK_ENABLED=true
CRAWL_FALLBACK_PROVIDERS=["apify", "brightdata"]
APIFY_API_KEY=
BRIGHTDATA_API_KEY=

# Epic 15 - Codebase Intelligence (Hallucination Detection)
# Threshold for blocking responses (0.0-1.0). Below threshold = allow
CODEBASE_HALLUCINATION_THRESHOLD=0.3
# Detector mode: 'warn' (log only) or 'block' (reject if threshold exceeded)
CODEBASE_DETECTOR_MODE=warn
# TTL for cached symbol tables in Redis (seconds)
CODEBASE_CACHE_TTL_SECONDS=3600
# Max symbols to keep in in-memory symbol table cache (0 disables the limit)
CODEBASE_SYMBOL_TABLE_MAX_SYMBOLS=200000
# Rate limits for indexing endpoints (per tenant)
CODEBASE_INDEX_RATE_LIMIT_MAX=10
CODEBASE_INDEX_RATE_LIMIT_WINDOW_SECONDS=3600
# Log warning when hallucination validation exceeds this duration (ms)
CODEBASE_DETECTION_SLOW_MS=2000
# Optional base path to constrain repo_path access for indexing/validation
# CODEBASE_ALLOWED_BASE_PATH=/home/chris/projects

# Epic 15 - Codebase Intelligence (RAG Context)
CODEBASE_RAG_ENABLED=false
CODEBASE_LANGUAGES=python,typescript,javascript
CODEBASE_EXCLUDE_PATTERNS=["**/node_modules/**","**/__pycache__/**","**/venv/**","**/.venv/**","**/dist/**","**/build/**","**/.git/**"]
CODEBASE_MAX_CHUNK_SIZE=1000
CODEBASE_INCLUDE_CLASS_CONTEXT=true
CODEBASE_INCREMENTAL_INDEXING=true
CODEBASE_INDEX_CACHE_TTL_SECONDS=86400

# Epic 20 - Memory Platform (Story 20-A1: Memory Scopes)
MEMORY_SCOPES_ENABLED=false
MEMORY_DEFAULT_SCOPE=session  # user | session | agent | global
MEMORY_INCLUDE_PARENT_SCOPES=true
MEMORY_CACHE_TTL_SECONDS=3600
MEMORY_MAX_PER_SCOPE=10000

# Epic 20 - Memory Platform (Story 20-A2: Memory Consolidation)
# Automatic memory consolidation: deduplication, decay, and cleanup
MEMORY_CONSOLIDATION_ENABLED=false
MEMORY_CONSOLIDATION_SCHEDULE="0 2 * * *"  # Cron: daily at 2 AM
MEMORY_SIMILARITY_THRESHOLD=0.9  # Threshold for duplicate detection (0.0-1.0)
MEMORY_DECAY_HALF_LIFE_DAYS=30  # Days for importance to halve
MEMORY_MIN_IMPORTANCE=0.1  # Minimum importance before removal (0.0-1.0)
MEMORY_CONSOLIDATION_BATCH_SIZE=100  # Memories to process per batch

# Epic 20 - Graph Intelligence (Story 20-B3: Query Routing)
# Routes queries to global (community-level) or local (entity-level) retrieval
QUERY_ROUTING_ENABLED=false
QUERY_ROUTING_USE_LLM=false  # Use LLM for ambiguous queries (when confidence < threshold)
QUERY_ROUTING_LLM_MODEL=gpt-4o-mini  # Model for LLM classification
QUERY_ROUTING_CONFIDENCE_THRESHOLD=0.7  # Threshold below which LLM/hybrid fallback is used

# Epic 21 - Voice I/O (Stories 21-E1, 21-E2)
# Enable speech-to-text (STT) and text-to-speech (TTS) features
VOICE_IO_ENABLED=false  # Master enable/disable for voice features
WHISPER_MODEL=base  # Whisper model size: tiny | base | small | medium | large
TTS_PROVIDER=openai  # TTS provider: openai | elevenlabs | pyttsx3
TTS_VOICE=alloy  # OpenAI voice: alloy | echo | fable | onyx | nova | shimmer
TTS_SPEED=1.0  # Speech speed (0.25-4.0)
ELEVENLABS_API_KEY=  # Required if TTS_PROVIDER=elevenlabs
